# Save the file as OpenCV-Ex1-Draw.py

import cv2
print(cv2.__version__)
import numpy as np

# create a black image of 500x500 pixels
img = np.zeros((500,500,3), np.uint8)
img = cv2.rectangle(img,(0,0),(500,500),(0,255,0),-1)

# Draw a horizontal red line with thickness of 5 px
img = cv2.line(img,(0,250),(500,250),(0,0,255),5)
# Draw a vertical red line with thickness of 5 px
img = cv2.line(img,(250,0),(250,500),(0,0,255),5)

# Draw a rectangle - top-left corner at (125,125) and
# bottom-right corner at (375,375), color: magenta, line thickness 3 px
img = cv2.rectangle(img,(125,125),(375,375),(255,0,255),3)

# Draw 4 circles of diameter 25, centers at 4 corners, line thickness 2 px
img = cv2.circle(img,(125,125), 25, (128,0,128), 2)
img = cv2.circle(img,(375,125), 25, (128,0,128), 2)
img = cv2.circle(img,(125,375), 25, (128,0,128), 2)
img = cv2.circle(img,(375,375), 25, (128,0,128), 2)

# Show everything on img
cv2.imshow('img',img)

cv2.waitKey(0)
# Save this file as OpenCV-Ex2-BounceBox.py

import cv2
#import numpy as np
print(cv2.__version__)

# you may need to change the number inside () to 0 1 or 2,
# depending on which webcam you are using
capture = cv2.VideoCapture(0)
# Below 2 lines are used to set the webcam window size
capture.set(3,640) # 3 is the width of the frame
capture.set(4,480) # 4 is the height of the frame

x = 0
dx = 1
# Start capturing and show frames on window named 'Frame'
while True:
    success, img = capture.read()
    cv2.rectangle(img, (x, 200), (x + 50, 250), (255, 255, 255), 2)
    x = x + dx
    if x >= 590 or x <= 0:
        dx = dx * (-1)

    cv2.imshow('Frame', img)
    if cv2.waitKey(20) & 0xff == ord('q'):
        break

capture.release()
cv2.destroyAllWindows()
# Save this file to your Github as OpenCV-Ex3-Haar-smile.py
import cv2
import numpy as np
print(cv2.__version__)

faceCascade = cv2.CascadeClassifier('Resources/haarcascade_frontalface_default.xml')
smileCascade = cv2.CascadeClassifier('Resources/haarcascade_smile.xml')

capture = cv2.VideoCapture('Resources/IU-edited.mp4')
# capture = cv2.VideoCapture(0)
capture.set(3,640)
capture.set(4,480)

font = cv2.FONT_HERSHEY_PLAIN

while True:
    success, img = capture.read()
    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = faceCascade.detectMultiScale(imgGray, 1.2, 10)
    for (x, y, w, h) in faces:
        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)
        roiImg = img[y:y + h, x:x + w]
        roiGray = imgGray[y:y + h, x:x + w]
        smiles = smileCascade.detectMultiScale(roiGray, 1.8, 8)
        for (sx, sy, sw, sh) in smiles:
            cv2.rectangle(roiImg, (sx, sy), (sx + sw, sy + sh), (0, 255, 0), 1)
            cv2.putText(img,'SMILE',(x,y),font,1.5, (0,0,255),2)

    cv2.imshow('Frame', img)
    #cv2.moveWindow('Frame', 100,20)
    if cv2.waitKey(1) == ord('q'):
        break

capture.release()
cv2.destroyAllWindows()
# Save this file to your Github as OpenCV-Ex4-Haar-ROI.py
import cv2
import numpy as np
print(cv2.__version__)

faceCascade = cv2.CascadeClassifier('Resources/haarcascade_frontalface_default.xml')

capture = cv2.VideoCapture('Resources/IU-edited.mp4')
# capture = cv2.VideoCapture(0)
capture.set(3,640)
capture.set(4,480)

font = cv2.FONT_HERSHEY_PLAIN

while True:
    success, img = capture.read()
    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # print(np.shape(imgGray))
    imgGray = cv2.cvtColor(imgGray, cv2.COLOR_GRAY2BGR)
    # print(np.shape(imgGray))
    faces = faceCascade.detectMultiScale(imgGray, 1.2, 10)
    for (x, y, w, h) in faces:
        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)
        roiImg = img[y:y + h, x:x + w].copy()
        imgGray[y:y + h, x:x + w]=roiImg

    cv2.imshow('Frame', imgGray)
    #cv2.moveWindow('Frame', 100,20)
    if cv2.waitKey(1) == ord('q'):
        break

capture.release()
cv2.destroyAllWindows()
# Save this file as OpenCV-1-Read.py

import cv2
print(cv2.__version__)

# read the image Lena.png and save into img
img = cv2.imread('Resources/lena.png')
# resize the image - img.shape[0] is the height, img.shape[1] is the width
img = cv2.resize(img, (int(img.shape[1]/1.5),int(img.shape[0]/1.5)))
# convert the BGR image to Grayscale
imgGray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

# Show the resized image and grayscale image onto two windows
cv2.imshow('Lena',img)
cv2.imshow('Gray Image', imgGray)

# wait for any key input to terminate the program
cv2.waitKey(0)
# Save this file as OpenCV-2-video.py

import cv2
#import numpy as np
print(cv2.__version__)

# Capture a video file
capture = cv2.VideoCapture('Resources/dog.mp4')

# Start capturing and show frames on window named 'Frame'
while True:
    success, img = capture.read()

    # below line is to resize video image frame, uncomment if you want to resize
    img = cv2.resize(img, (int(img.shape[1] / 1.5), int(img.shape[0] / 1.5)))

    cv2.imshow('Frame', img)
    if cv2.waitKey(20) & 0xff == ord('q'):
        break

capture.release()
cv2.destroyAllWindows()
# Save this file as OpenCV-3-webcam.py

import cv2
#import numpy as np
print(cv2.__version__)

# you may need to change the number inside () to 0 1 or 2,
# depending on which webcam you are using
capture = cv2.VideoCapture(0)
# Below 2 lines are used to set the webcam window size
capture.set(3,640) # 3 is the width of the frame
capture.set(4,480) # 4 is the height of the frame

# Start capturing and show frames on window named 'Frame'
while True:
    success, img = capture.read()

    cv2.imshow('Frame', img)
    if cv2.waitKey(20) & 0xff == ord('q'):
        break

capture.release()
cv2.destroyAllWindows()
# Save this file as OpenCV-4-Draw.py

import cv2
print(cv2.__version__)
import numpy as np

# create a black image of 500x500 pixels
img = np.zeros((500,500,3), np.uint8)

# Draw a diagonal blue line with thickness of 5 px
img = cv2.line(img,(0,0),(500,500),(255,0,0),5)

# Draw a circle of diameter 50, center at (400,100)
img = cv2.circle(img,(400,100), 50, (0,0,255), -1)

# Draw a rectangle - top-left corner and bottom-right corner of rectangle
img = cv2.rectangle(img,(250,0),(500,125),(0,255,0),3)

# Draw an ellipse: center, axis, angle, start angle, end angle, color, thickness
img = cv2.ellipse(img,(250,250),(100,50),0,0,180,(125,255,255),-1)

# Draw polygon
pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)
pts = pts.reshape((-1,1,2))
img = cv2.polylines(img,[pts],True,(0,255,255))

cv2.imshow('img',img)

cv2.waitKey(0)
# Save file as OpenCV-5-Text-on-image.py

import cv2
print(cv2.__version__)

# read the image Lena.png and save into img
img = cv2.imread('Resources/lena.png')
# resize the image - img.shape[0] is the height, img.shape[1] is the width
img = cv2.resize(img, (int(img.shape[1]/1.5),int(img.shape[0]/1.5)))

# Select font
font = cv2.FONT_HERSHEY_SIMPLEX
# Put a white text "Lena" on img
cv2.putText(img,'Lena',(10,300), font, 4,(255,255,255),2,cv2.LINE_AA)

# Show the resized image and grayscale image onto two windows
cv2.imshow('Lena',img)

# wait for any key input to terminate the program
cv2.waitKey(0)
# Save this file as OpenCV-6-Text-on-video.py

import cv2
#import numpy as np
print(cv2.__version__)

# Capture a video file
capture = cv2.VideoCapture('Resources/dog.mp4')

# Select font
font = cv2.FONT_HERSHEY_SIMPLEX

# Start capturing and show frames on window named 'Frame'
while True:
    success, img = capture.read()

    # below line is to resize video image frame, uncomment if you want to resize
    img = cv2.resize(img, (int(img.shape[1] / 1.5), int(img.shape[0] / 1.5)))

    # Put a red text on img
    cv2.putText(img, 'I am a cute dog!', (10, 400), font, 3, (0, 0, 255), 5)

    cv2.imshow('Frame', img)
    if cv2.waitKey(20) & 0xff == ord('q'):
        break

capture.release()
cv2.destroyAllWindows()
# Save this file as OpenCV-7-Text-on-webcam.py

import cv2
#import numpy as np
print(cv2.__version__)

# you may need to change the number inside () to 0 1 or 2,
# depending on which webcam you are using
capture = cv2.VideoCapture(0)
# Below 2 lines are used to set the webcam window size
capture.set(3,640) # 3 is the width of the frame
capture.set(4,480) # 4 is the height of the frame

# Select font
font = cv2.FONT_HERSHEY_SIMPLEX

# Start capturing and show frames on window named 'Frame'
while True:
    success, img = capture.read()

    # Put a red text on img
    cv2.putText(img, 'Hello! I am Winston Yeung!', (20, 50), font, 1.3, (0, 255, 0), 2)

    cv2.imshow('Frame', img)
    if cv2.waitKey(20) & 0xff == ord('q'):
        break

capture.release()
cv2.destroyAllWindows()
# Save this file as OpenCV-8-ImageProcessing.py

import cv2
import numpy as np
print(cv2.__version__)

# using a 5x5 kernel
kernel = np.ones((5,5),np.uint8)

# Read image from Resources folder
img = cv2.imread('Resources/lena.png')

# Resizing the image
img = cv2.resize(img, (int(img.shape[1]/1.5),int(img.shape[0]/1.5)))

# Convert BGR image to Grayscale
imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Convert Grayscale to Gaussian Blur
imgBlur = cv2.GaussianBlur(imgGray, (5,5), 0)

# Convert BGR image to Canny edges
imgCanny = cv2.Canny(img, 100, 100)

# Convert Canny edges image to dilated image
imgDilation = cv2.dilate(imgCanny, kernel, iterations=1)

# show different processed images on different windows
cv2.imshow('Lena',img)
cv2.imshow('Gray Image', imgGray)
cv2.imshow('Blur', imgBlur)
cv2.imshow('Canny', imgCanny)
cv2.imshow('Dilate', imgDilation)

# Wait for a key input from keyboard to terminate the program
cv2.waitKey(0)

# Save this file to your Github as OpenCV-9-Haar-face.py

# import libraries
import cv2
import numpy as np
print(cv2.__version__)

# Load the required xml file
faceCascade = cv2.CascadeClassifier('Resources/haarcascade_frontalface_default.xml')

# Read the image and change the image from BGR to grayscale
img = cv2.imread('Resources/lena.png')
imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Locate the features in the face(s) and return 4 values: x, y, width and height of the detected faces
faces = faceCascade.detectMultiScale(imgGray, 1.03, 5)

# draw rectangle on each face using the 4 values found for each face
for (x,y,w,h) in faces:
    cv2.rectangle(img, (x,y),(x+w,y+h),(255,0,0),2)

# Show the color image with rectangle on each face
cv2.imshow('Faces Detected',img)
cv2.waitKey(0)


# Save this file to your Github as OpenCV-10-Haar-face-eye.py

# import libraries
import cv2
import numpy as np
print(cv2.__version__)

# Load the required xml file to detect frontal face
faceCascade = cv2.CascadeClassifier('Resources/haarcascade_frontalface_default.xml')
# Load the required xml file to detect frontal face
eyeCascade = cv2.CascadeClassifier('Resources/haarcascade_eye.xml')

# Read the image and change the image from BGR to grayscale
img = cv2.imread('Resources/lena.png')
imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Locate the features in the face(s) and return 4 values: x, y, width and height of the detected faces
faces = faceCascade.detectMultiScale(imgGray, 1.03, 5)

# draw rectangle on each face using the 4 values found for each face
for (x,y,w,h) in faces:
    img = cv2.rectangle(img, (x,y),(x+w,y+h),(255,0,0),2)
    roiImg = img[y:y+h, x:x+w]
    roiGray = imgGray[y:y+h, x:x+w]
    eyes = eyeCascade.detectMultiScale(roiGray)
    for (ex, ey, ew, eh) in eyes:
        cv2.rectangle(roiImg, (ex,ey), (ex+ew,ey+eh),(0,255,0),1)

# Show the color image with rectangle on each face
cv2.imshow('Faces Detected',img)
cv2.waitKey(0)
# Save this file to your Github as OpenCV-11-Haar-face-video.py
import cv2
import numpy as np
print(cv2.__version__)

faceCascade = cv2.CascadeClassifier('Resources/haarcascade_frontalface_default.xml')

capture = cv2.VideoCapture('Resources/guitar.mp4')
# capture = cv2.VideoCapture(0)
capture.set(3,640)
capture.set(4,480)

while True:
    success, img = capture.read()
    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = faceCascade.detectMultiScale(imgGray, 1.1, 3)
    for (x, y, w, h) in faces:
        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)

    cv2.imshow('Frame', img)
    #cv2.moveWindow('Frame', 100,20)
    if cv2.waitKey(1) == ord('q'):
        break

capture.release()
cv2.destroyAllWindows()
# Save this file to your Github as OpenCV-12-ImageProcessing2.py

import cv2
print(cv2.__version__)
import numpy as np

img = cv2.imread('Resources/lena.png')
# resize the image - img.shape[0] is the height, img.shape[1] is the width
img = cv2.resize(img, (int(img.shape[1]/1.5),int(img.shape[0]/1.5)))
# convert the BGR image to Grayscale
imgGray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

roi = img[120:260, 120:260].copy()
roiGray = cv2.cvtColor(roi,cv2.COLOR_BGR2GRAY)
roiGray = cv2.cvtColor(roiGray,cv2.COLOR_GRAY2BGR)
img[120:260, 120:260] = roiGray

# Show the resized image and grayscale image onto two windows
cv2.imshow('Lena',img)
cv2.imshow('Gray Image', imgGray)
cv2.imshow('ROI Image', roi)
cv2.imshow('Gray ROI', roiGray)

# wait for any key input to terminate the program
cv2.waitKey(0)
# Save this file to your Github as OpenCV-13-FaceRec1.py

import cv2
print(cv2.__version__)
import face_recognition
print(face_recognition.__version__)

# Load and find the known images location and encode the face
img_Bill = face_recognition.load_image_file('Images_Known/Bill Gates.jpg')
img_Bill = cv2.cvtColor(img_Bill, cv2.COLOR_RGB2BGR)
faceLoc_Bill = face_recognition.face_locations(img_Bill)[0]
encode_Bill = face_recognition.face_encodings(img_Bill)[0]
print(faceLoc_Bill)
cv2.rectangle(img_Bill, (faceLoc_Bill[3],faceLoc_Bill[0]),(faceLoc_Bill[1],faceLoc_Bill[2]),(255,0,255),2)

# Load and find unknown images, encode, and compare to known faces
imgTest = face_recognition.load_image_file('Images_Unknown/bill-gates-2.jpg')
imgTest = cv2.cvtColor(imgTest, cv2.COLOR_RGB2BGR)
faceLocTest = face_recognition.face_locations(imgTest)[0]
encodeTest = face_recognition.face_encodings(imgTest)[0]

cv2.rectangle(imgTest, (faceLocTest[3],faceLocTest[0]),(faceLocTest[1],faceLocTest[2]),(255,0,0),2)

# Compare images
results = face_recognition.compare_faces([encode_Bill],encodeTest)
face_dist = face_recognition.face_distance([encode_Bill],encodeTest)
print(results)
print(face_dist)

cv2.imshow('Bill Gates',img_Bill)
cv2.imshow('Bill Gates Test image', imgTest)

cv2.waitKey(0)
# Save this file to your Github as OpenCV-14-FaceRec2.py

import cv2
print(cv2.__version__)
import face_recognition
print(face_recognition.__version__)

# Load and find the known images location and encode the face
img_wy = face_recognition.load_image_file('Images_Known/wy-1.png')
img_wy = cv2.cvtColor(img_wy, cv2.COLOR_RGB2BGR)
faceLoc_wy = face_recognition.face_locations(img_wy)[0]
encode_wy = face_recognition.face_encodings(img_wy)[0]

capture = cv2.VideoCapture(1)
capture.set(3,640)
capture.set(4,480)

while True:
    success, img = capture.read()
    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    # From webcam, read frame and find unknown face, encode, and compare to known faces
    faceLocCam = face_recognition.face_locations(imgRGB)[0]
    encodeCam = face_recognition.face_encodings(imgRGB)[0]

    cv2.rectangle(img, (faceLocCam[3], faceLocCam[0]), (faceLocCam[1], faceLocCam[2]), (255, 0, 0), 2)

    # Compare images
    results = face_recognition.compare_faces([encode_wy], encodeCam)
    print(results)
    cv2.imshow('Frame', img)
    #cv2.moveWindow('Frame', 100,20)
    if cv2.waitKey(1) == 27:
        break

capture.release()
cv2.destroyAllWindows()
# Save this file to your Github as OpenCV-15-FaceRec3.py

import cv2
print(cv2.__version__)
import face_recognition
print(face_recognition.__version__)
import numpy as np

# Load and find the known images location and encode the face
imgKnown = face_recognition.load_image_file('Images_Known/wy-1.png')
imgKnown = cv2.cvtColor(imgKnown, cv2.COLOR_RGB2BGR)
faceLocKnown = face_recognition.face_locations(imgKnown)[0]
encodeKnown = face_recognition.face_encodings(imgKnown)[0]

capture = cv2.VideoCapture(1)
capture.set(3,640)
capture.set(4,480)

while True:
    success, img = capture.read()
    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    # From webcam, read and find unknown images, encode, and compare to known faces
    faceLocsCam = face_recognition.face_locations(imgRGB)
    encodesCam = face_recognition.face_encodings(imgRGB, faceLocsCam)

    # Compare faces in webcam to encoded face
    for (top,right,bottom,left),encodeCam in zip(faceLocsCam,encodesCam):
        results = face_recognition.compare_faces([encodeKnown],encodeCam)
        faceDist = face_recognition.face_distance([encodeKnown],encodeCam)
        print(faceDist)
        if faceDist < 0.4:
                cv2.rectangle(img, (left,top),(right,bottom), (255, 0, 0), 2)

    cv2.imshow('Frame', img)
    #cv2.moveWindow('Frame', 100,20)
    if cv2.waitKey(1) == 27:
        break

capture.release()
cv2.destroyAllWindows()
import cv2
import numpy as np

def nothing(): pass

cv2.namedWindow('Trackbars')

cv2.createTrackbar('HueLow','Trackbars',0,179,nothing)
cv2.createTrackbar('HueHigh','Trackbars',150,179,nothing)
cv2.createTrackbar('SatLow','Trackbars',0,255,nothing)
cv2.createTrackbar('SatHigh','Trackbars',255,255,nothing)
cv2.createTrackbar('ValLow','Trackbars',0,255,nothing)
cv2.createTrackbar('ValHigh','Trackbars',255,255,nothing)


# Set up webcam
# cam = cv2.VideoCapture(1)
# cam.set(3,640)
# cam.set(4,480)

# Start capturing and show frames on window
while True:
    # success, img = cam.read()
    img = cv2.imread('Resources/smarties.png')
    cv2.imshow('Frame', img)
    # cv2.moveWindow('Frame', 100,20)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    hueLow = cv2.getTrackbarPos('HueLow','Trackbars')
    hueHigh = cv2.getTrackbarPos('HueHigh', 'Trackbars')
    satLow = cv2.getTrackbarPos('SatLow', 'Trackbars')
    satHigh = cv2.getTrackbarPos('SatHigh', 'Trackbars')
    valLow = cv2.getTrackbarPos('ValLow', 'Trackbars')
    valHigh = cv2.getTrackbarPos('ValHigh', 'Trackbars')

    FGmask = cv2.inRange(hsv, (hueLow,satLow,valLow),(hueHigh,satHigh,valHigh))
    cv2.imshow('FGmask',FGmask)

    FG = cv2.bitwise_and(img,img,mask=FGmask)
    cv2.imshow('FG', FG)

    BGmask = cv2.bitwise_not(FGmask)
    cv2.imshow('BGmask', BGmask)

    BG = cv2.cvtColor(BGmask,cv2.COLOR_GRAY2BGR)

    finalImg = cv2.add(FG,BG)
    cv2.imshow('FinalImage', finalImg)

    if cv2.waitKey(1) == ord('q'):
        break

# cam.release()
cv2.destroyAllWindows()
import cv2
import numpy as np

def nothing(): pass

cv2.namedWindow('Trackbars')

cv2.createTrackbar('HueLow','Trackbars',24,179,nothing)
cv2.createTrackbar('HueHigh','Trackbars',86,179,nothing)
cv2.createTrackbar('SatLow','Trackbars',139,255,nothing)
cv2.createTrackbar('SatHigh','Trackbars',255,255,nothing)
cv2.createTrackbar('ValLow','Trackbars',122,255,nothing)
cv2.createTrackbar('ValHigh','Trackbars',255,255,nothing)


# Set up webcam
cam = cv2.VideoCapture(1)
# cam.set(3,640)
# cam.set(4,480)

# Start capturing and show frames on window
while True:
    success, img = cam.read()

    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    hueLow = cv2.getTrackbarPos('HueLow','Trackbars')
    hueHigh = cv2.getTrackbarPos('HueHigh', 'Trackbars')
    satLow = cv2.getTrackbarPos('SatLow', 'Trackbars')
    satHigh = cv2.getTrackbarPos('SatHigh', 'Trackbars')
    valLow = cv2.getTrackbarPos('ValLow', 'Trackbars')
    valHigh = cv2.getTrackbarPos('ValHigh', 'Trackbars')

    FGmask = cv2.inRange(hsv, (hueLow,satLow,valLow),(hueHigh,satHigh,valHigh))
    cv2.imshow('FGmask',FGmask)
    final = cv2.bitwise_and(img,img,mask=FGmask)

    contours, hierarchy = cv2.findContours(FGmask,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)
    contours = sorted(contours,key=lambda x:cv2.contourArea(x),reverse=True)
    for cnt in contours:
        area = cv2.contourArea(cnt)
        (x,y,w,h) = cv2.boundingRect(cnt)
        if area > 100:
            #cv2.drawContours(img,[cnt],0,(255,0,0),3)
            cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),3)

    cv2.imshow('Frame', img)
    cv2.imshow('Final',final)
    if cv2.waitKey(1) == 27:
        break

# cam.release()
cv2.destroyAllWindows()
